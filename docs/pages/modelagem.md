# 04 - Modelagem

<a href="../../README.md" title="Voltar para a p√°gina principal">
üè† Voltar para Home
</a>

## üéØ Vis√£o Geral

Este notebook marca o in√≠cio da fase de **modelagem preditiva**, utilizando modelos cl√°ssicos de Machine Learning para estabelecer um baseline s√≥lido antes de avan√ßar para algoritmos mais sofisticados.

Aqui come√ßamos a responder √† pergunta central do projeto:

> **‚ÄúCom base nas informa√ß√µes operacionais, conseguimos prever se um pedido ir√° atrasar (fl_atraso_cli)?‚Äù**

Para isso, utilizamos os dados j√° tratados e enriquecidos (Feature Engineering) nos notebooks anteriores:

- `00_eda_tratamento.ipynb`  
- `01_eda_descritiva.ipynb`  
- `02_eda_inferencial.ipynb`  
- `03_preprocessing_pipeline.ipynb`  

---

## üìå Objetivos deste Notebook

Este notebook implementa:

1. **Modelo Baseline (DummyClassifier)**  
   - estabelece uma linha de compara√ß√£o m√≠nima para avaliar se os modelos realmente t√™m poder preditivo.

2. **Modelos Cl√°ssicos de Classifica√ß√£o**
   - **Regress√£o Log√≠stica**  
   - **√Årvore de Decis√£o (Decision Tree Classifier)**  

3. **Integra√ß√£o com o Pipeline de Pr√©-processamento**  
   - garante que imputa√ß√£o, codifica√ß√£o e escala sejam aplicadas corretamente dentro de cada modelo.

4. **Valida√ß√£o e M√©tricas**
   - Accuracy
   - Precision
   - Recall
   - F1-score
   - ROC AUC
   - Classification Report
   - Matriz de Confus√£o

5. **Compara√ß√£o entre modelos**
   - para selecionar o melhor candidato a ser refinado no notebook 05 (modelos avan√ßados).

## üìä M√©tricas de Avalia√ß√£o de Modelos de Classifica√ß√£o

### (Accuracy, Precision, Recall, F1-Score, ROC AUC)

Ao avaliar modelos de Machine Learning para **classifica√ß√£o**, utilizamos m√©tricas que medem diferentes aspectos da performance ‚Äî cada uma adequada para um tipo de problema e custo de erro.

Abaixo segue a explica√ß√£o **detalhada, profissional e did√°tica**, para voc√™ colocar diretamente no arquivo **modelagem.md**.

---

## 1. Acur√°cia (Accuracy)

### üìå **O que mede**

A propor√ß√£o de todas as previs√µes corretas:

[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
]

### üìå **Quando usar**

* Quando as classes est√£o **balanceadas**.
* Quando **todos os erros t√™m o mesmo custo**.

### üìå **Como interpretar**

* Valores pr√≥ximos de **1.0** ‚Üí excelente desempenho.
* Pode ser **enganosa em dados desbalanceados**.

**Exemplo**:
Se apenas 10% dos pedidos atrasam, um modelo que sempre diz ‚Äún√£o vai atrasar‚Äù tem 90% de acur√°cia ‚Äî mas √© in√∫til.

---

## 2. Precis√£o (Precision)

### üìå **O que mede**

Entre todas as previs√µes **positivas**, quantas estavam realmente corretas?

[
\text{Precision} = \frac{TP}{TP + FP}
]

### üìå **Quando usar**

* Quando o custo de um **falso positivo** √© alto.
* Exemplos:

  * Diagn√≥stico de doen√ßas graves com exames caros.
  * Detec√ß√£o de fraude (alertas falsos t√™m custo).
  * OTIF: avisar atraso quando n√£o h√° atraso ‚Üí gera ru√≠do operacional.

### üìå **Como interpretar**

* Alta precis√£o = o modelo quase n√£o ‚Äúaluga‚Äù falso alarme.

---

## 3. Recall (Sensibilidade)

### üìå **O que mede**

Entre todos os casos positivos reais, quantos o modelo identificou?

[
\text{Recall} = \frac{TP}{TP + FN}
]

### üìå **Quando usar**

* Quando o custo de um **falso negativo** √© muito alto.
* Exemplos:

  * Perder uma fraude
  * Perder paciente realmente doente
  * No OTIF: perder um atraso real √© **muito pior** que dizer que vai atrasar √† toa

### üìå **Como interpretar**

* Alto recall = o modelo consegue capturar a maioria dos atrasos reais.

---

## 4. F1-Score

### üìå **O que mede**

A m√©dia harm√¥nica entre **precis√£o** e **recall**:

[
\text{F1} = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
]

### üìå **Quando usar**

* Quando voc√™ quer **uma √∫nica m√©trica equilibrada** entre precis√£o e recall.
* Quando o dataset √© **desbalanceado**.
* Quando voc√™ quer comparar modelos de forma justa.

### üìå **Como interpretar**

* F1 alto = modelo equilibrado
* F1 baixo = modelo falha em precis√£o, recall ou ambos

---

## 5. AUC ROC (√Årea sob a Curva ROC)

### üìå **O que mede**

Avalia a capacidade do modelo de separar classes **em todos os thresholds poss√≠veis**.

[
0.5 = \text{modelo aleat√≥rio} \
1.0 = \text{separa√ß√£o perfeita}
]

### üìå **Quando usar**

* Avalia√ß√£o geral do modelo.
* Compara√ß√£o entre modelos.
* √ìtimo para visualizar qualidade da separa√ß√£o.

### üìå **Limita√ß√µes**

* Pode ser otimista em datasets extremamente desbalanceados.
* Menos intuitiva para explicar ao time de neg√≥cio em compara√ß√£o ao Precision/Recall.

---

## üéØ Resumo Profissional das M√©tricas

| M√©trica       | O que mede                         | Quando usar                 | Risco                        |
| ------------- | ---------------------------------- | --------------------------- | ---------------------------- |
| **Accuracy**  | % de previs√µes corretas            | Classes balanceadas         | Enganosa em desbalanceamento |
| **Precision** | Confiabilidade dos positivos       | Falso positivo custa caro   | Baixa pode gerar ru√≠do       |
| **Recall**    | % de positivos capturados          | Falso negativo custa caro   | Alta pode reduzir precis√£o   |
| **F1**        | Equil√≠brio entre Precis√£o e Recall | Compara√ß√£o justa de modelos | Pode esconder detalhes       |
| **ROC AUC**   | Separa√ß√£o geral das classes        | Comparar modelos            | Otimista em desbalanceamento |

---

## Como se relacionam

* **Precision ‚Üë** ‚Üí menos falsos positivos
* **Recall ‚Üë** ‚Üí menos falsos negativos
* **F1 alto** ‚Üí bom equil√≠brio
* **ROC AUC alto** ‚Üí maior separabilidade estrutural

No seu projeto OTIF:

üìå **Recall √© extremamente importante** ‚Äî pois perder atrasos reais seria devastador.
üìå **Precision tamb√©m √© importante** ‚Äî mas secund√°rio.
üìå **F1 resume a performance.**
üìå **ROC AUC confirma a capacidade estrutural do modelo.**

## Modelos

### Modelo Baseline - DummyClassifier

**Descri√ß√£o:** O modelo baseline tem como objetivo estabelecer um ponto de refer√™ncia m√≠nimo para compara√ß√£o dos modelos reais.
Neste trabalho, ser√° utilizado o DummyClassifier(strategy="most_frequent"), que sempre prev√™ a classe majorit√°ria encontrada no dataset.

#### **Nota T√©cnica:** 

Esses valores s√£o esperados, j√° que o modelo nunca prev√™ ‚Äú1 = atraso‚Äù, apenas repete a classe dominante (0).
O comportamento demonstra, vi√©s extremo para a classe majorit√°ria, incapacidade total de detectar atrasos e desempenho equivalente a uma classifica√ß√£o aleat√≥ria (ROC AUC = 0.5)

**Import√¢ncia do Baseline:** A fun√ß√£o do baseline n√£o √© prever corretamente, mas sim, estabelecer o ponto m√≠nimo aceit√°vel de desempenho, garantir que um modelo real n√£o seja pior do que adivinhar sempre ‚Äún√£o atraso‚Äù, servir como refer√™ncia para medir ganho de performance e deixar expl√≠cito o impacto do desbalanceamento da classe alvo.

**Conclus√£o**

O baseline confirma que o dataset possui forte desbalanceamento, ent√£o m√©tricas como precision, recall e F1 ser√£o fundamentais, com isso √© prov√°vel que qualquer modelo √∫til deve superar esse resultado, principalmente em recall para a classe "1" (atraso)

Assim, os modelos seguintes devem ser avaliados n√£o apenas por accuracy, mas tamb√©m por F1, Recall e ROC AUC, que de fato capturam a qualidade da detec√ß√£o de atrasos.

### Treinando a Regress√£o Log√≠stica (LogisticRegression)

**Descri√ß√£o:** A regress√£o log√≠stica √© uma t√©cnica estat√≠stica para prever a probabilidade de um evento ocorrer, com base em vari√°veis independentes.

#### **Nota T√©cnica:** 
A Regress√£o Log√≠stica apresentou excelente desempenho no problema de classifica√ß√£o de atraso (OTIF), especialmente ao considerar o desbalanceamento natural da base.

**Principais m√©tricas obtidas:**

- **Accuracy:** 0.7082
- **Precision:** 0.4728
- **Recall:** 0.7279
- **F1 Score:** 0.5732
- **ROC AUC:** 0.7879

**Interpreta√ß√£o:**
- **O recall de 72%** significa que o modelo identifica a maior parte dos pedidos atrasados, caracter√≠stica fundamental em cen√°rios log√≠sticos.
- **A precision de 47%**, em bases desbalanceadas, √© extremamente elevada e indica bom poder discriminativo.
- **O F1 Score acima de 57%** mostra bom equil√≠brio entre precis√£o e recall.
- **O ROC AUC de 79%** evidencia capacidade preditiva robusta, muito acima do esperado para um modelo linear.

**Conclus√£o**
A Regress√£o Log√≠stica estabeleceu um baseline forte, mostrando que o processo de pr√©-processamento e as features derivadas capturam bem o comportamento operacional dos atrasos.
Este modelo servir√° como refer√™ncia para avaliar t√©cnicas mais avan√ßadas, como Random Forest, Gradient Boosting e XGBoost.

### 5. √Årvore de Decis√£o (DecisionTreeClassifier)

**Descri√ß√£o:** Uma DecisionTreeClassifier √© um modelo de aprendizado supervisionado que usa uma estrutura de √°rvore para fazer previs√µes. √â usada para classifica√ß√£o e funciona como um fluxograma de regras de decis√£o, onde cada n√≥ representa um teste de atributo, cada ramo √© o resultado do teste e os n√≥s-folha representam a classe final ou a previs√£o.

#### **Nota T√©cnica:** 
A √Årvore de Decis√£o apresentou um desempenho surpreendentemente robusto no problema de classifica√ß√£o de atrasos log√≠sticos, alcan√ßando:

- **Accuracy:** 0.893
- **Precision:** 0.779
- **Recall:** 0.840
- **F1 Score:** 0.809
- **ROC AUC:** 0.896

Esses resultados indicam equil√≠brio entre a capacidade de identificar atrasos (recall) e evitar alarmes falsos (precision).  

O modelo capturou com sucesso padr√µes operacionais estruturais, muito influenciados pelas novas features de engenharia criadas no EDA inferencial, em especial o lead_time_total_horas, complexidade_operacional e pedido_grande_flag.

O uso de class_weight="balanced" aliado ao pipeline completo de pr√©-processamento contribuiu significativamente para reduzir vi√©s e estabilizar a performance do modelo.

Os resultados superam amplamente o baseline e j√° se aproximam de modelos ensemble como Random Forest ou Gradient Boosting, indicando forte separabilidade inerente aos dados.

### 6. Random Forest Classifier (RandomForestClassifier)

**Descri√ß√£o:** Random Forest √© um algoritmo de aprendizado de m√°quina que combina m√∫ltiplas √°rvores de decis√£o para melhorar a precis√£o e a estabilidade das previs√µes. Ele funciona treinando cada √°rvore em diferentes partes aleat√≥rias dos dados e depois combinando os resultados por vota√ß√£o (para classifica√ß√£o) ou m√©dia (para regress√£o). Esse m√©todo ajuda a reduzir o risco de overfitting (quando o modelo se ajusta demais aos dados de treinamento) e √© amplamente usado tanto para tarefas de classifica√ß√£o quanto de regress√£o. 

#### **Nota T√©cnica:** 
O modelo Random Forest apresentou desempenho superior na maior parte das m√©tricas relevantes para o problema de previs√£o de atrasos operacionais. Com um ROC AUC de 0.9576, o modelo demonstrou excelente capacidade de separa√ß√£o entre pedidos atrasados e entregues no prazo.

O recall de 0.9093 foi o maior entre os modelos testados, indicando que o Random Forest √© o mais eficaz em identificar pedidos que ir√£o atrasar, a caracter√≠stica mais importante no contexto log√≠stico, onde falsos negativos (atrasos n√£o detectados) s√£o extremamente prejudiciais.

Embora o accuracy (0.8839) seja levemente inferior ao Decision Tree, o modelo compensou com uma maior sensibilidade e com uma robustez significativamente maior na generaliza√ß√£o. O F1 Score de 0.8084 refor√ßa o equil√≠brio entre precis√£o e recall.

O comportamento do Random Forest tamb√©m √© mais est√°vel, reduzindo riscos de overfitting quando comparado √† √°rvore de decis√£o individual. A combina√ß√£o de desempenho forte, robustez estat√≠stica e alta capacidade de separa√ß√£o coloca o Random Forest como o modelo mais consistente do conjunto at√© o momento.

### 7. Gradient Boosting (GradientBoostingClassifier)

**Descri√ß√£o:** Gradient Boosting √© um algoritmo de aprendizado de m√°quina que cria um modelo forte e preciso combinando v√°rios modelos mais fracos (geralmente √°rvores de decis√£o) sequencialmente. Cada modelo subsequente foca em corrigir os erros cometidos pelo modelo anterior, minimizando uma fun√ß√£o de perda (o erro) por meio de um processo iterativo que usa o gradiente para guiar as melhorias. √â amplamente utilizado em tarefas de classifica√ß√£o e regress√£o.

#### **Nota T√©cnica:** 
O Gradient Boosting √© um algoritmo de ensemble baseado na constru√ß√£o sequencial de √°rvores fracas, onde cada nova √°rvore tenta corrigir os erros da √°rvore anterior.
Em muitos cen√°rios, esse modelo supera t√©cnicas como Random Forest e Decision Tree, especialmente quando os padr√µes s√£o sutis e os dados t√™m pouco ru√≠do.

No entanto, seu desempenho depende fortemente da natureza da base. Em contextos log√≠sticos, caracterizados por alta variabilidade, caudas pesadas e outliers, o GBM tende a perder estabilidade.

---

## **3. Resultados Obtidos**

| M√©trica       | Valor  |
| ------------- | ------ |
| **Accuracy**  | 0.8191 |
| **Precision** | 0.7681 |
| **Recall**    | 0.4699 |
| **F1-score**  | 0.5831 |
| **ROC AUC**   | 0.8838 |

---

## **4. Interpreta√ß√£o dos Resultados**

### **4.1 Pontos fortes**

* **Boa precision (76%)**: quando o modelo prev√™ atraso, ele acerta muitas vezes.
* **Bom AUC (0.88)**: indica uma capacidade razo√°vel de discriminar entre atrasos e n√£o atrasos.

---

### **4.2 Pontos fracos (cr√≠ticos)**

O principal problema est√° no **Recall = 46%**, o mais baixo entre todos os modelos avaliados.

**Recall baixo significa:**

> O modelo n√£o identifica grande parte dos pedidos que realmente atrasam.

Em problemas log√≠sticos **isso √© inaceit√°vel**.
A empresa precisa **prever atrasos reais**, mesmo que isso gere alguns falsos positivos.
O custo de um atraso real n√£o previsto √© muito maior do que o custo de prever atraso quando n√£o existe.

Assim, apesar de algumas m√©tricas satisfat√≥rias, o modelo apresenta uma **falha estrutural no objetivo de neg√≥cio**.

---

### **4.3 Compara√ß√£o com outros modelos**

O Gradient Boosting se mostrou **inferior ao Random Forest e √† Decision Tree** em m√©tricas essenciais:

#### ‚úî Random Forest

* Recall: **0.9093**
* F1: **0.8084**
* ROC AUC: **0.9576**

#### ‚úî Decision Tree

* Recall: **0.8407**
* F1: **0.8092**

#### ‚úò Gradient Boosting

* Recall: **0.4699**
* F1: **0.5831**

**Conclus√£o:**

> Apesar de tecnicamente robusto, o Gradient Boosting n√£o se ajustou bem ao comportamento da base, perdendo desempenho nas m√©tricas mais relevantes para o problema.

---

## **5. Conclus√£o T√©cnica**

O Gradient Boosting apresentou resultados s√≥lidos em algumas m√©tricas, mas mostrou **desempenho insuficiente para o objetivo operacional**, deixando escapar grande parte dos atrasos reais.

Por esse motivo:

### **O modelo N√ÉO √© adequado como candidato final.**

### Os modelos **Random Forest** e **Decision Tree** apresentam performance superior e maior ader√™ncia ao objetivo do neg√≥cio.

## 8. Comparando os Melhores Modelos

O melhor modelo global foi o Random Forest, porque ele entrega o melhor balan√ßo entre precis√£o, recall, estabilidade e capacidade preditiva real.

**Por que N√ÉO escolhemos o Decision Tree, apesar da accuracy maior?**
O Decision Tree apresentou accuracy ligeiramente maior (0.8932), mas isso foi artificial, pois √© muito provavel que tenha sofrido overfitting, porque √°rvores singulares tendem a memorizar o dataset e o ROC AUC dele (0.8956) √© bem inferior ao do Random Forest (0.9576), mostrando que sua capacidade de generaliza√ß√£o √© menor.

**Por que N√ÉO escolhemos Gradient Boosting?**
Ele teve desempenho bom em precis√£o, mas recall extremamente baixo (0.47).
Para um problema de opera√ß√£o log√≠stica, perder atrasos reais √© inaceit√°vel ent√£o recall √© crucial.

**Por que N√ÉO escolhemos Regress√£o Log√≠stica?**
√â um modelo simples, baseline, e serviu para confirmar que modelos lineares n√£o capturam bem a complexidade do processo log√≠stico.

O Random Forest apresentou o melhor trade-off entre precis√£o, recall e capacidade de generaliza√ß√£o, com o maior ROC AUC entre todos os modelos. Isso indica que ele distingue muito bem atrasos de n√£o atrasos e por isso, ele foi selecionado como modelo final para otimiza√ß√µes e an√°lise de threshold.

### 9. Tratamento de Desbalanceamento (Class Weights e SMOTE)

Modelos treinados sem compensa√ß√£o tendem a priorizar a classe majorit√°ria, ent√£o, para mitigar isso, tr√™s t√©cnicas foram avaliadas:

---

**4.1 ‚Äî Class Weights (Regress√£o Log√≠stica)**

O class_weight="balanced" ajusta o peso das classes automaticamente, penalizando mais os erros na classe minorit√°ria.  
O resultado aumenta sensivelmente o *recall*, mas mant√©m *precision* baixa, t√≠pico de modelos lineares.  
√â um baseline v√°lido, por√©m inferior aos modelos baseados em √°rvores.

---

**4.2 ‚Äî SMOTE + Random Forest (Oversampling)**

SMOTE cria amostras sint√©ticas da classe minorit√°ria, equilibra o dataset e melhora o aprendizado do modelo.  
O Random Forest treinado ap√≥s o oversampling apresentou:

- **Accuracy:** 92,11%  
- **Precision:** 84,63%  
- **Recall:** 86,40%  
- **F1:** 85,50%  
- **ROC AUC:** 0,9704  

Este foi o melhor desempenho entre todos os modelos testados.
O SMOTE reduziu o vi√©s da classe majorit√°ria sem causar overfitting e permitiu que o Random Forest aprendesse padr√µes mais refinados sobre atrasos.

---

**4.3 ‚Äî Random UnderSampling**

Reduz o n√∫mero de exemplos da classe majorit√°ria.
O recall aumentou fortemente (94%), mas houve queda na precision (72%), indicando aumento de falsos positivos.  
Foi √∫til como compara√ß√£o, por√©m inferior ao modelo SMOTE + Random Forest.

---

**Conclus√£o T√©cnica**

O modelo **Random Forest + SMOTE** apresentou o melhor equil√≠brio entre precis√£o, recall e separabilidade (AUC).  
Este modelo ser√° utilizado como refer√™ncia para as pr√≥ximas etapas de otimiza√ß√£o de threshold e interpretabilidade.

### 10. Thresholds de Decis√£o

## Nota T√©cnica

### **1. O que √© o Threshold?**

Modelos de classifica√ß√£o bin√°ria como Random Forest, Logistic Regression e Gradient Boosting **n√£o predizem diretamente classes**.
Eles predizem **probabilidades**.

Exemplo:

```
Probabilidade de atraso = 0.73 ‚Üí modelo acha 73% de chance de atrasar
```

Para converter essa probabilidade em uma decis√£o **(0 = no prazo / 1 = atraso)**, usamos um **threshold (limiar)**.

O threshold padr√£o √© **0.50**, mas ele **n√£o √© necessariamente o melhor ponto** para o neg√≥cio.

---

### **2. Por que variar o Threshold?**

Porque alterar o threshold muda o equil√≠brio entre:

* **Precision** ‚Üí qu√£o corretas s√£o as previs√µes de atraso
* **Recall** ‚Üí quantos atrasos reais o modelo consegue detectar
* **F1-score** ‚Üí equil√≠brio entre os dois

**Threshold baixo (ex.: 0.10)**

* Recall alt√≠ssimo (detecta quase todos atrasos)
* Precision baixa (gera muitos falsos alarmes)

**Threshold alto (ex.: 0.85)**

* Precision alt√≠ssima (quando alerta, √© certeiro)
* Recall muito baixo (quase n√£o detecta atrasos)

---

### **3. Como realizamos a an√°lise**

Testamos thresholds de **0.10 a 0.90**, e para cada valor calculamos:

* Precision
* Recall
* F1-score

Geramos tanto:

* uma **tabela comparativa dos thresholds**,
* quanto o **gr√°fico das curvas Precision √ó Recall √ó F1**.

Esses dois artefatos confirmaram o comportamento cl√°ssico esperado:

- Precision sobe conforme threshold aumenta
- Recall cai conforme threshold aumenta
- F1 tem um **pico** ‚Äî esse √© o threshold ideal

---

### **4. Resultado da An√°lise**

Tabela (trecho mais relevante):

| threshold | precision | recall    | f1        |
| --------- | --------- | --------- | --------- |
| 0.30      | 0.691     | 0.960     | 0.804     |
| 0.40      | 0.774     | 0.925     | 0.843     |
| **0.50**  | **0.846** | **0.863** | **0.855** |
| 0.60      | 0.904     | 0.768     | 0.831     |
| 0.70      | 0.946     | 0.635     | 0.760     |

O **melhor F1-score (0.855)** ocorreu exatamente no threshold **0.50**.

---

### **5. Conclus√£o da Otimiza√ß√£o**

**O threshold √≥timo para este problema √© 0.50**, pois:

* mant√©m **alta capacidade de captura dos atrasos reais (recall = 0.863)**
* mant√©m **boa precis√£o (precision = 0.846)**
* e atinge o **maior F1-score entre todos os thresholds testados**

Ou seja, **√© o ponto de equil√≠brio ideal entre sensibilidade e precis√£o**, maximizando o desempenho global do modelo.

---

### **6. Justificativa para o neg√≥cio**

> *‚ÄúNo contexto operacional da log√≠stica, perder atrasos reais tem alto custo. Por isso, o threshold escolhido privilegia um balan√ßo entre alertas corretos (precision) e detec√ß√£o de atrasos reais (recall). O threshold de 0.50 foi selecionado por maximizar o F1-score e garantir o melhor equil√≠brio operacional.‚Äù*

---

### **7. Conclus√£o final da nota t√©cnica**

A an√°lise de threshold demonstrou que o desempenho do modelo pode ser significativamente ajustado ao cen√°rio real de opera√ß√£o. O threshold **n√£o √© um valor fixo**, mas sim um **par√¢metro estrat√©gico** que deve ser calibrado conforme:

* custos de erro,
* prioridades do neg√≥cio,
* e comportamento real dos dados.

Neste trabalho, o threshold de **0.50** foi comprovadamente o valor que **maximiza o desempenho preditivo** e **melhor atende ao contexto da Zenatur**.

---

<div align="left">
  <a href="#topo" title="Voltar ao in√≠cio do README">‚¨ÜÔ∏è Topo</a>
</div>

---
