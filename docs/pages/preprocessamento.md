# 03 - PrÃ©-Processamento e ConstruÃ§Ã£o do Pipeline (scikit-learn)

<a href="../../README.md" title="Voltar para a pÃ¡gina principal">
ğŸ  Voltar para Home
</a>

Este documento descreve **todas as etapas de prÃ©-processamento** aplicadas ao dataset **acompanhamento_operacional_FE.csv**, conforme implementadas no notebook `03_preprocessing_pipeline.ipynb`.
O objetivo Ã© garantir **qualidade, consistÃªncia, ausÃªncia de data leakage e reprodutibilidade total** para uso em modelos de Machine Learning.

---

## ğŸ”§ **1. Carregamento do Dataset**

Carregamos o dataset processado apÃ³s o EDA e Feature Engineering:

```
df = pd.read_csv("../database/processed/acompanhamento_operacional_FE.csv")
```

Esse arquivo jÃ¡ contÃ©m:

* Valores tratados (nulos resolvidos no EDA)
* Tipos ajustados
* Features criadas no arquivo FE (ex.: lead_time_total, complexidade_operacional, flags binÃ¡rias)

---

## ğŸ” **2. IdentificaÃ§Ã£o dos Tipos de VariÃ¡veis**

SeparaÃ§Ã£o correta entre **variÃ¡veis numÃ©ricas e categÃ³ricas**:

* **numÃ©ricas:** todas as `int64` e `float64`
* **categÃ³ricas:** todas as `object`

```
num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.tolist()
```

â–¶ï¸ **Por que isso Ã© importante?**
Porque cada tipo de variÃ¡vel exige um tratamento diferente no pipeline (imputaÃ§Ã£o, escala, codificaÃ§Ã£o).

---

## ğŸ§© **3. ImputaÃ§Ã£o de Valores Ausentes**

Mesmo apÃ³s a limpeza inicial, garantimos **robustez** usando imputadores padronizados dentro do pipeline.

### âœ” NumÃ©ricas â†’ *Mediana*

```
numeric_imputer = SimpleImputer(strategy="median")
```

**Motivo:**

* A mediana Ã© robusta contra outliers
* Adequada para dados operacionais que tÃªm caudas longas e valores extremos

### âœ” CategÃ³ricas â†’ *Moda (most_frequent)*

```
categorical_imputer = SimpleImputer(strategy="most_frequent")
```

**Motivo:**

* Evita criar categorias artificiais
* MantÃ©m consistÃªncia operacional
* Funciona muito bem com colunas como UF, modalidade, sigla_cliente etc.

---

## ğŸ”„ **4. CodificaÃ§Ã£o CategÃ³rica (Encoding)**

Utilizamos **OneHotEncoder**, que transforma cada categoria em uma coluna binÃ¡ria.

```
categorical_encoder = OneHotEncoder(
    handle_unknown="ignore",
    sparse_output=False
)
```

### Por que usar OneHot?

* Modelos clÃ¡ssicos (LogReg, RF, GBM) funcionam melhor com dados numÃ©ricos
* Evita atribuir pesos falsos (como ocorreria com Label Encoding)
* `handle_unknown="ignore"` evita erros em inferÃªncias reais

---

## ğŸ“ **5. Escalonamento de VariÃ¡veis NumÃ©ricas (Scaling)**

Usamos **StandardScaler**, que transforma todas as features numÃ©ricas para:

* mÃ©dia = 0
* desvio padrÃ£o = 1

```
numeric_scaler = StandardScaler()
```

### Por que isso Ã© importante?

* Modelos como **RegressÃ£o LogÃ­stica** e **SVM** sÃ£o sensÃ­veis Ã  escala
* Evita que features com grande magnitude dominem o modelo
* Melhora convergÃªncia e estabilidade matemÃ¡tica
* Mesmo Random Forest e Gradient Boosting, que nÃ£o precisam de scaling, **nÃ£o sÃ£o prejudicados** pela padronizaÃ§Ã£o

---

## ğŸ—ï¸ **6. Montagem do ColumnTransformer**

Aqui unimos todas as etapas anteriores em um bloco Ãºnico que aplica automaticamente:

### Pipeline numÃ©rico:

* imputaÃ§Ã£o (mediana)
* StandardScaler

### Pipeline categÃ³rico:

* imputaÃ§Ã£o (moda)
* OneHotEncoder

```
preprocess = ColumnTransformer(transformers=[
    ("num_pipeline", Pipeline(steps=[
        ("imputer", numeric_imputer),
        ("scaler", numeric_scaler)
    ]), num_cols),

    ("cat_pipeline", Pipeline(steps=[
        ("imputer", categorical_imputer),
        ("encoder", categorical_encoder)
    ]), cat_cols)
])
```

ğŸ“Œ **Essa Ã© a etapa que evita data leakage.**

Por quÃª?

â¡ Tudo Ã© aplicado *dentro do pipeline*,
â¡ que sÃ³ roda usando **apenas os dados de treino durante o fit**,
â¡ garantindo que nenhuma informaÃ§Ã£o dos dados de teste vaza para o treinamento.

---

## âœ‚ï¸ **7. Split Train/Test**

Antes de treinar qualquer modelo:

```
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=42
)
```

### Cuidados importantes:

* **stratify=y** â†’ preserva a proporÃ§Ã£o de atrasos/no atrasos
* **random_state=42** â†’ garante reprodutibilidade total
* O split ocorre antes do `fit()` do pipeline â†’ **evita vazamento total**

---

## ğŸ”— **8. Pipeline Final de PrÃ©-Processamento**

Criamos um pipeline final reutilizÃ¡vel para qualquer modelo:

```
pipeline_preprocess = Pipeline(steps=[
    ("preprocess", preprocess)
])
```

### O que esse pipeline faz automaticamente?

Para cada coluna:

| Etapa     | NumÃ©ricas      | CategÃ³ricas |
| --------- | -------------- | ----------- |
| ImputaÃ§Ã£o | Mediana        | Moda        |
| Escala    | StandardScaler | â€”           |
| Encoding  | â€”              | OneHot      |

â¡ Tudo ocorre automaticamente durante o `.fit()` e `.transform()`
â¡ Sem necessidade de mexer manualmente na base
â¡ Zero risco de data leakage

---

# ğŸ“¦ **Resumo Geral do PrÃ©-Processamento**

### âœ” Correto para ML

### âœ” EscalÃ¡vel

### âœ” ReutilizÃ¡vel

### âœ” Blindado contra vazamento de dados

### âœ” Segue o padrÃ£o scikit-learn exigido em projetos profissionais

O pipeline estÃ¡ pronto para ser conectado diretamente ao notebook de modelagem (`04_modelagem.ipynb`), permitindo que modelos como:

* Logistic Regression
* Decision Tree
* Random Forest
* Gradient Boosting
* SMOTE Pipelines
* Class Weights

sejam treinados **com o mesmo processo padronizado e robusto**.
---

<div align="left">
  <a href="#topo" title="Voltar ao inÃ­cio do README">â¬†ï¸ Topo</a>
</div>

---